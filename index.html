<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Kaiwen Zhang</title>

    <meta name="author" content="Kaiwen Zhang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Kaiwen Zhang
                </p>
                <p>
                  Hi! I'm currently a Ph.D. student at <a href="https://www.mmlab-ntu.com/index.html/">MMLab@NTU</a>, advised by Prof. <a href="https://xingangpan.github.io/">Xingang Pan</a>.
                  Previously, I received my B.Eng. degree in Computer Science and Technology with highest honors from <a href="https://www.tsinghua.edu.cn/">Tsinghua University</a> in 2025.                  I also had a broad research internship experience at <a href="https://www.bytedance.com/en/">Bytedance / Tiktok US</a>, <a href="https://www.horizon.auto/">Horizon Robotics</a> and <a href="https://www.shlab.org.cn/">Shanghai AI Lab</a>.
                  My research is supported by the prestigious <a href="https://www.ntu.edu.sg/admissions/graduate/financialmatters/scholarships/npgs">Nanyang President's Graduate Scholarship</a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:sze68zkw@gmail.com">Email</a> &nbsp;/&nbsp;
                  <!-- <a href="data/KaiwenZhang-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="data/KaiwenZhang-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?user=OmW2XCwAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/kevin-thu/">Github</a> &nbsp;/&nbsp;
                  <a href="https://x.com/sze68zkw/">X (Twitter)</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/Kevin.jpeg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/Kevin.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My primary research interests lie in computer vision and generative models, with a specific focus on <b>video generation</b> and <b>world models</b>. 
                  My long-term goal is to build knowledgeable, controllable, and creative machine intelligence that possess a deep understanding of the physical world and have the capability to imagine beyond the world.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 6px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
            <td style="padding:16px;width:20%;vertical-align:middle">
              <div class="one">
                <div class="two">
                  <video autoplay loop muted controls width="100%">
                    <source src='images/StoryMem.mp4' type='video/mp4'>
                  </video>
                </div>
              </div>
              <!-- <script type="text/javascript">
                function radmesh_start() {
                  document.getElementById('radmesh_image').style.opacity = "1";
                }

                function radmesh_stop() {
                  document.getElementById('radmesh_image').style.opacity = "0";
                }
                radmesh_stop()
              </script> -->
            </td>
            <td style="padding:8px;width:80%;vertical-align:middle">
              <a href="https://kevin-thu.github.io/homepage/">
                <span class="papertitle">StoryMem: Multi-shot Long Video Storytelling with Memory</span>
              </a>
              <br>
              <b>Kaiwen Zhang</b>, Liming Jiang, Angtian Wang, Jacob Zhiyuan Fang, Tiancheng Zhi, Qing Yan, Hao Kang, Xin Lu, Xingang Pan
              <br>
              <b><em>Under Review</em></b>
              <br>
              <!-- <a href="https://kevin-thu.github.io/Epona/">Project Page</a>
              &nbsp;/&nbsp;
              <a href="https://arxiv.org/abs/2506.24113">Paper</a>
              &nbsp;/&nbsp;
              <a href="https://github.com/Kevin-thu/Epona">Code</a> &nbsp;<img src="https://img.shields.io/github/stars/Kevin-thu/Epona.svg" alt="GitHub stars"> -->
              <!-- <p>
                We present Epona, a novel autoregressive diffusion world model for autonomous driving. 
                Epona is a latent diffusion model that generates realistic and diverse scenes from a latent code, which is conditioned on the past frames and the future action.
              </p> -->
            </td>
          </tr>

          <tr>
            <td style="padding:16px;width:20%;vertical-align:middle">
              <div class="one">
                <div class="two">
                  <video autoplay loop muted controls width="100%">
                    <source src='images/Epona.mp4' type='video/mp4'>
                  </video>
                </div>
              </div>
            </td>
            <td style="padding:8px;width:80%;vertical-align:middle">
              <a href="https://kevin-thu.github.io/Epona/">
                <span class="papertitle">Epona: Autoregressive Diffusion World Model for Autonomous Driving</span>
              </a>
              <br>
              <b>Kaiwen Zhang</b>*, Zhenyu Tang*, Xiaotao Hu, Xingang Pan, Xiaoyang Guo, Yuan Liu, Jingwei Huang, Li Yuan, Qian Zhang, Xiaoxiao Long, Xun Cao, Wei Yin (*Equal contribution)
              <br>
              <b><em>ICCV 2025</em></b>
              <br>
              <a href="https://kevin-thu.github.io/Epona/">Project Page</a>
              &nbsp;/&nbsp;
              <a href="https://arxiv.org/abs/2506.24113">Paper</a>
              &nbsp;/&nbsp;
              <a href="https://github.com/Kevin-thu/Epona">Code</a> &nbsp;<img src="https://img.shields.io/github/stars/Kevin-thu/Epona.svg" alt="GitHub stars">
            </td>
          </tr>

          <tr>
            <td style="padding:16px;width:20%;vertical-align:middle">
              <div class="one">
                <div class="two">
                  <video autoplay loop muted controls width="100%">
                    <source src='images/DiffMorpher.mp4' type='video/mp4'>
                  </video>
                </div>
              </div>
            </td>
            <td style="padding:8px;width:80%;vertical-align:middle">
              <a href="https://kevin-thu.github.io/DiffMorpher_page/">
                <span class="papertitle">DiffMorpher: Unleashing the Capability of Diffusion Models for Image Morphing                </span>
              </a>
              <br>
              <b>Kaiwen Zhang</b>, Yifan Zhou, Xudong Xu, Xingang Pan<sup>✉</sup>, Bo Dai (<sup>✉</sup>Corresponding author)
              <br>
              <b><em>CVPR 2024</em></b>
              <br>
              <a href="https://kevin-thu.github.io/DiffMorpher_page/">Project Page</a>
              &nbsp;/&nbsp;
              <a href="https://arxiv.org/abs/2312.07409">Paper</a>
              &nbsp;/&nbsp;
              <a href="https://github.com/Kevin-thu/DiffMorpher">Code</a> &nbsp;<img src="https://img.shields.io/github/stars/Kevin-thu/DiffMorpher.svg" alt="GitHub stars">
            </td>
          </tr>

          <tr>
            <td style="padding:16px;width:20%;vertical-align:middle">
              <div class="one">
                <div class="two">
                  <img src="images/EmbodiedWM.png" alt="EmbodiedWM" style="width:100%;max-width:100%;">
                </div>
              </div>
            </td>
            <td style="padding:8px;width:80%;vertical-align:middle">
              <a href="https://github.com/NJU3DV-LoongGroup/Embodied-World-Models-Survey/">
                <span class="papertitle">A Survey: Learning Embodied Intelligence from Physical Simulators and World Models</span>
              </a>
              <br>
              Xiaoxiao Long*, Qingrui Zhao*, <b>Kaiwen Zhang</b>*, Zihao Zhang*, Dingrui Wang*, Yumeng Liu*, Zhengjie Shu*, Yi Lu*, Shouzheng Wang*, Xinzhe Wei*, Wei Li, Wei Yin, Yao Yao, Jia Pan, Qiu Shen, Ruigang Yang, Xun Cao, Qionghai Dai (*Equal contribution)
              <br>
              <b><em>Preprint 2025</em></b>
              <br>
              <!-- <a href="https://kevin-thu.github.io/DiffMorpher_page/">Project Page</a>
              &nbsp;/&nbsp; -->
              <a href="https://arxiv.org/abs/2507.00917">Paper</a>
              &nbsp;/&nbsp;
              <a href="https://github.com/NJU3DV-LoongGroup/Embodied-World-Models-Survey">Github</a> &nbsp;<img src="https://img.shields.io/github/stars/NJU3DV-LoongGroup/Embodied-World-Models-Survey.svg" alt="GitHub stars">
            </td>
          </tr>


          </tbody></table>

          
					<table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Experience</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
           
            <tr>
              <td align="center" style="padding:16px;width:25%;vertical-align:middle">
                <img src="images/ByteDance.png" alt="Bytedance" style="width:100%;max-width:100%;object-fit: cover;">
              </td>
              <td style="padding:8px;width:75%;vertical-align:middle">
                <strong>Research Intern, Bytedance / TikTok Intelligent Creation Lab, San Jose, US</strong>
                <br>
                2025.07 - 2025.12
                <br>
                Collaborators: <a href="https://liming-jiang.com/">Liming Jiang</a>, <a href="https://scholar.google.com/citations?user=mFC0wp8AAAAJ&hl=en/">Xin Lu</a>, <a href="https://angtianwang.github.io/">Angtian Wang</a>, <a href="https://tiancheng-zhi.github.io/">Tiancheng Zhi</a>, etc.
              </td>              
            </tr>

            <tr>
              <td align="center" style="padding:16px;width:25%;vertical-align:middle">
                <img src="images/horizonrobotics.webp" alt="Horizon Robotics" style="width:100%;max-width:100%;object-fit: cover;">
              </td>
              <td style="padding:8px;width:75%;vertical-align:middle">
                <strong>Research Intern, Horizon Robotics, Beijing, China</strong>
                <br>
                2024.09 - 2025.06
                <br>
                Collaborators: <a href="https://www.xxlong.site/">Prof. Xiaoxiao Long</a>, <a href="https://yvanyin.xyz/">Wei Yin</a>, etc.
              </td>
            </tr>

            <tr>
              <td align="center" style="padding:16px;width:25%;vertical-align:middle">
                <img src="images/SAIL.png" alt="Shanghai AI Lab" style="width:100%;max-width:100%;object-fit: cover;">
              </td>
              <td style="padding:8px;width:75%;vertical-align:middle">
                <strong>Research Intern, Shanghai AI Lab, Shanghai, China</strong>
                <br>
                2023.07 - 2024.01
                <br>
                Collaborators: <a href="https://xingangpan.github.io/">Prof. Xingang Pan</a>, <a href="https://daibo.info/">Prof. Bo Dai</a>, etc.
              </td>
            </tr>

            <tr>
              <td align="center" style="padding:16px;width:25%;vertical-align:middle">
                <img src="images/tsinghua.png" alt="Tsinghua University" style="width:100%;max-width:100%;object-fit: cover;">
              </td>
              <td style="padding:8px;width:75%;vertical-align:middle">
                <strong>Research Assistant, Knowledge Engineering Group (KEG), Tsinghua University, Beijing, China</strong>
                <br>
                2022.09 - 2023.06
                <br>
                Collaborators: <a href="https://keg.cs.tsinghua.edu.cn/jietang/">Prof. Jie Tang</a>, <a href="https://scholar.google.com/citations?user=JSEzrlwAAAAJ&hl=en/">Wenyi Hong</a>, etc.
              </td>
            </tr>

            <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
              <tr>
                <td>
                  <h2>Honors & Awards</h2>
                </td>
              </tr>
            </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px 16px;width:100%;vertical-align:middle">
                <ul style="margin:0; padding-left:20px;">
                  <li><b>Nanyang President's Graduate Scholarship</b>, 2025</li>
                  <li><b>Outstanding Graduate of Beijing</b> (Top 3%), 2025</li>
                  <li><b>Outstanding Graduate of Tsinghua University</b> (Top 10%), 2025</li>
                  <li>Academic Excellence Scholarship, Tsinghua University, 2024</li>
                  <li>China Computer Federation (CCF) Outstanding Student, 2024</li>
                  <li><b>SenseTime Scholarship</b> (25 undergraduates all over China), 2024</li>
                  <li><b>National Scholarship</b> (Highest honor for undergraduates in China), 2023</li>
                  <li>Second Prize (3 / 109), The Jittor AI Challenge (Image Generation Track), 2023</li>
                  <li>Comprehensive Excellence Scholarship, Tsinghua University, 2022</li>
                  <li>Meritorious Winner, American Mathematical Contest in Modeling, 2022</li>
                </ul>
              </td>
            </tr>
          </tbody></table>

          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  Design and source code from Jon Barron's <a href="https://jonbarron.info/">website</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
